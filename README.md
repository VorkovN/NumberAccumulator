<b>О проекте:</b>

Проект NumberAccumulator содержит в себе 2 сервиса:
1. Client
2. Server

Суть проекта заключается в том, что пользователь на клиенте вводит любую строчку,
а сервер ищет в этой строчке все целые числа(как положительные, так и отрицательные), сортирует их и находит сумму
Выход из режима ввода осуществляется через последовательный ввод ctrl+c и Enter.

<b>О реализации:</b>

Так как сервсы предполагались небольшими, то вполне разумно использовать следующую модель архитектуры:
* Главным классом приложения является Фасад. Фасад управляет всеми потоками и циклами. Только методы фасада доступны из main
Именно фасад решает когда заставлять какой класс работать:

Если фасад решает, что пришло время прочитать входящие данные, то запрашивает у транспорта чтение из сокета

Если фасад решает, что пришло время прочитать из консоли, то запрашивает метод чтения из консоли

Если фасад решает, что пришло время обработать пришедшие данные, то запрашивает у библиотеки выполнение алгоритмов

Если фасад решает, что пришло время записать входящие данные, то запрашивает у транспорта отправку данных

Можно было обойтись и просто добавлением интерфейса фасада в поля транспорта, но
реализованный подход упрощает логику приложения, так как низкоуровневые модули не будут (пусть даже и через интерфейс) вызывать методы высокоуровневых модулей

* По рекомендации из указаний к проекту, было сокращено кол-во потоков до минимума. Для клиента это 1 поток, для сервера 2(для udp и tcp соединений)
* В клиенте на сокет была повешана опция прерывания ожидания приема сообщения(по умолчанию 5 сек), чтобы избежать возможности зависнуть при поломке сервера
* На серверере у разных, параллельно работающих транспортов, нет общих ресурсов, а потому получилось избежать накладных расходов на синхронизацию потоков
* На сервере в качетве организации управления tcp соединениями был выбран инструмент epoll. В отличии от poll и select этот инструмент возвращает только те сокеты, для которых произошло событие
* Чтобы сокет серверный сокет был доступен сразу после завершения работы, на него была навешана опция reuse_addr
* Чтобы не возникло такой ситуации, что ожидание ответа от сервера закончилось по таймауту, а после совершения нового запроса прилетел ответ на старый запрос и сломал очередность запрос-ответов,
был введен счетчик сообщений для каждого клиента. Тем самым клиент будет полностью игнорировать пакеты с устаревшим каунтером и будет ждать ответ только на последний запрос

<b>О запуске и тестировании:</b>

Для тестирования были написаны unit тесты и функциональные:
* <u>Все скрипты в программе следует запускать из директории, в которой они лежат</u>
* Собрать бинари можно через скрипты buildRelease.sh и buildDebug.sh в папке [**Scripts**](./Scripts)
* По умолчанию при сборке создается директория bin в корневой директории, также эту директорию можно собрать при нажатии кнопки install в CLion
* При запуске в режиме дебаг соберутся еще и бинари unit тестов, которые можно запустить и проверить код
* Сценарии к функциональным тестам описаны в  [**соответствующем ридми**](./Tests/FunctionalTests/README.md)
* Функциональные тесты используют бинари из директории bin, потому перед запуском функциональных тестов необходимо собрать проект описанными выше методами


